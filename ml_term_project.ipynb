{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Term Project: Implementing ML Models from Scratch\n",
        "## Machine Learning / Pattern Recognition\n",
        "### FINAL INTERNATIONAL UNIVERSITY | Fall 2025-26\n",
        "\n",
        "**Student Name:**  \n",
        "**Student ID:**  \n",
        "**Due Date:** 27 December 2025 (Saturday) before midnight\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this project, you will implement three fundamental machine learning models **from scratch**:\n",
        "1. **Linear Regression** using gradient descent\n",
        "2. **Logistic Regression** using gradient descent (binary classification)\n",
        "3. **Decision Tree** using information gain (multi-class classification)\n",
        "\n",
        "You will use the **Iris dataset** for all three tasks:\n",
        "- **Linear Regression**: Predict `petal_width` from other features\n",
        "- **Logistic Regression**: Classify `setosa` vs `non-setosa` (binary)\n",
        "- **Decision Tree**: Classify all three species (multi-class)\n",
        "\n",
        "**Important Rules:**\n",
        "- ‚úÖ **Allowed**: `sklearn` for dataset loading, `KFold`, and evaluation metrics\n",
        "- ‚ùå **Not Allowed**: `sklearn` models (e.g., `LinearRegression`, `LogisticRegression`, `DecisionTreeClassifier`)\n",
        "- ‚ùå **Not Allowed**: `GridSearchCV` or similar libraries for hyperparameter search\n",
        "- All models must be implemented using **native Python and NumPy only**\n",
        "\n",
        "**Grading:**\n",
        "- Linear Regression Implementation: **20 pts**\n",
        "- Linear Regression 5-Fold CV & Results: **10 pts**\n",
        "- Logistic Regression Implementation: **20 pts**\n",
        "- Logistic Regression 5-Fold CV & Results: **10 pts**\n",
        "- Decision Tree Implementation: **20 pts**\n",
        "- Decision Tree 5-Fold CV & Results: **10 pts**\n",
        "- Written Questions (3 √ó 4 pts): **12 pts** \n",
        "- **Total: 100 + 2 pts**\n",
        "\n",
        "**Integrity Test:**\n",
        "- **Date**: 3 January 2026 during lecture hours\n",
        "- **Format**: Short quiz on paper\n",
        "- **Pass (‚â•80%)**: Marks awarded directly from PDF\n",
        "- **Fail (<80%)**: Demo session required\n",
        "- **No Show**: 50% penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "checklist"
      },
      "source": [
        "---\n",
        "\n",
        "## Pre-Submission Checklist\n",
        "\n",
        "- [ ] Name and student ID at top\n",
        "- [ ] No cells are added or removed\n",
        "- [ ] All TODO sections completed\n",
        "- [ ] All questions answered\n",
        "- [ ] Code runs without errors\n",
        "- [ ] Results tables included with mean, std, and 95% CI\n",
        "- [ ] Hyperparameter tuning plots included\n",
        "- [ ] Run All before saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Sklearn - ONLY for dataset, splitting, and metrics\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_header"
      },
      "source": [
        "## Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X_full = iris.data  # Features: sepal_length, sepal_width, petal_length, petal_width\n",
        "y_full = iris.target  # Labels: 0=setosa, 1=versicolor, 2=virginica\n",
        "\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"  Total samples: {X_full.shape[0]}\")\n",
        "print(f\"  Features: {iris.feature_names}\")\n",
        "print(f\"  Classes: {iris.target_names}\")\n",
        "print(f\"\\nClass distribution: {dict(zip(iris.target_names, np.bincount(y_full)))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helper_header"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helpers"
      },
      "outputs": [],
      "source": [
        "def compute_confidence_interval(values, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Compute mean, std, and 95% confidence interval.\n",
        "    \n",
        "    Args:\n",
        "        values: List or array of values from k folds\n",
        "        confidence: Confidence level (default 0.95)\n",
        "    \n",
        "    Returns:\n",
        "        mean, std, (lower_bound, upper_bound)\n",
        "    \"\"\"\n",
        "    values = np.array(values)\n",
        "    n = len(values)\n",
        "    mean = np.mean(values)\n",
        "    std = np.std(values, ddof=1)  # Sample std\n",
        "    \n",
        "    # t-value for 95% CI with n-1 degrees of freedom (approx 2.776 for n=5)\n",
        "    t_value = 2.776  # For 5 folds, df=4\n",
        "    margin = t_value * (std / np.sqrt(n))\n",
        "    \n",
        "    return mean, std, (mean - margin, mean + margin)\n",
        "\n",
        "\n",
        "def print_results(metric_name, values):\n",
        "    \"\"\"Print results with mean, std, and 95% CI.\"\"\"\n",
        "    mean, std, ci = compute_confidence_interval(values)\n",
        "    print(f\"{metric_name}: {mean:.4f} ¬± {std:.4f} (95% CI: [{ci[0]:.4f}, {ci[1]:.4f}])\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "task1_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 1: Linear Regression (30 points)\n",
        "\n",
        "Implement Linear Regression using gradient descent to predict `petal_width` from the other three features (`sepal_length`, `sepal_width`, `petal_length`).\n",
        "\n",
        "**Hyperparameters to tune:**\n",
        "- Learning rate: `[0.001, 0.01, 0.1]`\n",
        "- Regularisation strength (L2): `[0.01, 0.1]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "linreg_class_header"
      },
      "source": [
        "## 1.1: Implement LinearRegression Class (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "linreg_class"
      },
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "    \"\"\"\n",
        "    Linear Regression using gradient descent.\n",
        "    \n",
        "    Attributes:\n",
        "        learning_rate: Step size for gradient descent\n",
        "        n_iterations: Number of training iterations (fixed = 500)\n",
        "        reg_strength: L2 regularisation strength (0 = no regularisation)\n",
        "        weights: Model weights (including bias)\n",
        "        train_losses: List of training losses per iteration\n",
        "        val_losses: List of validation losses per iteration\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_iterations=500, learning_rate=0.01, reg_strength=0.0):\n",
        "        \"\"\"\n",
        "        Initialise the Linear Regression model.\n",
        "        \n",
        "        Args:\n",
        "            learning_rate: Step size for gradient descent\n",
        "            n_iterations: Number of training iterations\n",
        "            reg_strength: L2 regularisation strength\n",
        "        \"\"\"\n",
        "        # TODO: Store hyperparameters and initialise attributes (3 points)\n",
        "        pass\n",
        "    \n",
        "    def _add_bias(self, X):\n",
        "        \"\"\"Add a column of ones for the bias term.\"\"\"\n",
        "        # TODO: Add bias column to X (2 points)\n",
        "        # Hint: Use np.c_ or np.column_stack\n",
        "        pass\n",
        "    \n",
        "    def _compute_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute Mean Squared Error loss with optional L2 regularisation.\n",
        "        \n",
        "        Loss = (1/2n) * sum((y_pred - y)^2) + (reg_strength/2) * sum(weights^2)\n",
        "        \n",
        "        Note: Do not regularise the bias term.\n",
        "        \"\"\"\n",
        "        # TODO: Implement MSE loss with L2 regularisation (2 points)\n",
        "        pass\n",
        "    \n",
        "    def _compute_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute gradient of the loss function.\n",
        "        \n",
        "        Gradient = (1/n) * X.T @ (X @ weights - y) + reg_strength * weights\n",
        "        \n",
        "        Note: Do not regularise the bias term.\n",
        "        \"\"\"\n",
        "        # TODO: Implement gradient computation (2 points)\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Train the model using gradient descent.\n",
        "        \n",
        "        Args:\n",
        "            X_train: Training features\n",
        "            y_train: Training targets\n",
        "            X_val: Validation features (optional, for tracking val loss)\n",
        "            y_val: Validation targets (optional)\n",
        "        \n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        # TODO: Implement training loop (8 points)\n",
        "        # Steps:\n",
        "        # 1. Add bias to X_train (and X_val if provided)\n",
        "        # 2. Initialise weights to zeros\n",
        "        # 3. For each iteration:\n",
        "        #    a. Compute gradient\n",
        "        #    b. Update weights\n",
        "        #    c. Record train loss\n",
        "        #    d. Record val loss if X_val provided\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions.\n",
        "        \n",
        "        Args:\n",
        "            X: Features\n",
        "        \n",
        "        Returns:\n",
        "            Predictions\n",
        "        \"\"\"\n",
        "        # TODO: Add bias and compute predictions (3 points)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "linreg_cv_header"
      },
      "source": [
        "## 1.2: 5-Fold Cross-Validation for Linear Regression (10 points)\n",
        "\n",
        "**Steps:**\n",
        "1. Prepare data: Use features [0, 1, 2] to predict feature [3] (petal_width)\n",
        "2. For each fold:\n",
        "   - Split train into train_inner and validation (80%-20%)\n",
        "   - Grid search over hyperparameters using validation set\n",
        "   - Train final model with best hyperparameters on full train set\n",
        "   - Evaluate on test set\n",
        "3. Report results with mean, std, and 95% CI\n",
        "4. Plot train/val loss curves for one fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "linreg_cv"
      },
      "outputs": [],
      "source": [
        "# Prepare data for Linear Regression\n",
        "# Features: sepal_length, sepal_width, petal_length\n",
        "# Target: petal_width\n",
        "X_linreg = X_full[:, :3]  # First 3 features\n",
        "y_linreg = X_full[:, 3]   # 4th feature as target\n",
        "\n",
        "# Standardise features\n",
        "scaler = StandardScaler()\n",
        "X_linreg = scaler.fit_transform(X_linreg)\n",
        "\n",
        "print(f\"Linear Regression Data:\")\n",
        "print(f\"  X shape: {X_linreg.shape}\")\n",
        "print(f\"  y shape: {y_linreg.shape}\")\n",
        "\n",
        "# Hyperparameter grid\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "reg_strengths = [0.01, 0.1]\n",
        "\n",
        "# Storage for results\n",
        "mse_scores = []\n",
        "r2_scores = []\n",
        "best_params_per_fold = []\n",
        "\n",
        "# For plotting (save from one fold)\n",
        "plot_train_losses = None\n",
        "plot_val_losses = None\n",
        "plot_fold = 1\n",
        "\n",
        "# 5-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LINEAR REGRESSION: 5-FOLD CROSS-VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for fold_num, (train_idx, test_idx) in enumerate(kf.split(X_linreg), 1):\n",
        "    print(f\"\\n--- Fold {fold_num} ---\")\n",
        "    \n",
        "    # Split data\n",
        "    X_train_full, X_test = X_linreg[train_idx], X_linreg[test_idx]\n",
        "    y_train_full, y_test = y_linreg[train_idx], y_linreg[test_idx]\n",
        "    \n",
        "    # TODO: Further split train into train_inner and validation (80%-20%) (1 points)\n",
        "    # Hint: Use simple index slicing or another KFold split\n",
        "    \n",
        "    # TODO: Grid search over hyperparameters (3 points)\n",
        "    # For each combination of (learning_rate, reg_strength):\n",
        "    #   1. Train model on train_inner\n",
        "    #   2. Evaluate on validation set\n",
        "    #   3. Track best parameters based on validation MSE\n",
        "    \n",
        "    # TODO: Save train/val losses from one hyperparameter setting for plotting (1 points)\n",
        "    # (Pick fold 1 and the middle hyperparameter values)\n",
        "    \n",
        "    # TODO: Train final model with best parameters on full training set (1 points)\n",
        "    \n",
        "    # TODO: Evaluate on test set and store results (1 points)\n",
        "    \n",
        "    pass\n",
        "\n",
        "# TODO: Print final results with mean, std, and 95% CI (1 points)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LINEAR REGRESSION: FINAL RESULTS\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "linreg_plot"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot training and validation loss curves (1 points)\n",
        "# Use plot_train_losses and plot_val_losses saved from fold 1\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "# TODO: Create the plot (1 points)\n",
        "# plt.plot(...)\n",
        "# plt.xlabel('Iteration')\n",
        "# plt.ylabel('MSE Loss')\n",
        "# plt.title('Linear Regression: Training vs Validation Loss')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question1"
      },
      "source": [
        "**Question 1:** Explain how L2 regularisation affects the weights during training. (4 points, 4-5 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "answer1"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "task2_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 2: Logistic Regression (30 points)\n",
        "\n",
        "Implement Logistic Regression using gradient descent for binary classification. You will classify `setosa` (class 0) vs `non-setosa` (classes 1 and 2).\n",
        "\n",
        "**Hyperparameters to tune:**\n",
        "- Learning rate: `[0.001, 0.01, 0.1]`\n",
        "- Regularisation strength (L2): `[0.01, 0.1]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "logreg_class_header"
      },
      "source": [
        "## 2.1: Implement LogisticRegression Class (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "logreg_class"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "    Logistic Regression using gradient descent for binary classification.\n",
        "    \n",
        "    Attributes:\n",
        "        learning_rate: Step size for gradient descent\n",
        "        n_iterations: Number of training iterations (fixed 500)\n",
        "        reg_strength: L2 regularisation strength\n",
        "        weights: Model weights (including bias)\n",
        "        train_losses: List of training losses per iteration\n",
        "        val_losses: List of validation losses per iteration\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, n_iterations=500, reg_strength=0.0):\n",
        "        \"\"\"\n",
        "        Initialise the Logistic Regression model.\n",
        "        \n",
        "        Args:\n",
        "            learning_rate: Step size for gradient descent\n",
        "            n_iterations: Number of training iterations\n",
        "            reg_strength: L2 regularisation strength\n",
        "        \"\"\"\n",
        "        # TODO: Store hyperparameters and initialise attributes\n",
        "        pass\n",
        "    \n",
        "    def _add_bias(self, X):\n",
        "        \"\"\"Add a column of ones for the bias term.\"\"\"\n",
        "        # TODO: Add bias column to X (2 points)\n",
        "        pass\n",
        "    \n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Compute sigmoid function.\n",
        "        \n",
        "        sigmoid(z) = 1 / (1 + exp(-z))\n",
        "        \n",
        "        Hint: Clip z to avoid overflow (e.g., np.clip(z, -500, 500))\n",
        "        \"\"\"\n",
        "        # TODO: Implement sigmoid (2 points)\n",
        "        pass\n",
        "    \n",
        "    def _compute_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute binary cross-entropy loss with optional L2 regularisation.\n",
        "        \n",
        "        Loss = -(1/n) * sum(y*log(p) + (1-y)*log(1-p)) + (reg_strength/2) * sum(weights^2)\n",
        "        \n",
        "        Hint: Use np.clip on probabilities to avoid log(0)\n",
        "        Note: Do not regularise the bias term.\n",
        "        \"\"\"\n",
        "        # TODO: Implement cross-entropy loss with L2 regularisation (2 points)\n",
        "        pass\n",
        "    \n",
        "    def _compute_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute gradient of the loss function.\n",
        "        \n",
        "        Gradient = (1/n) * X.T @ (sigmoid(X @ weights) - y) + reg_strength * weights\n",
        "        \n",
        "        Note: Do not regularise the bias term.\n",
        "        \"\"\"\n",
        "        # TODO: Implement gradient computation (2 points)\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Train the model using gradient descent.\n",
        "        \n",
        "        Args:\n",
        "            X_train: Training features\n",
        "            y_train: Training labels (0 or 1)\n",
        "            X_val: Validation features (optional)\n",
        "            y_val: Validation labels (optional)\n",
        "        \n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        # TODO: Implement training loop (8 points)\n",
        "        # Steps:\n",
        "        # 1. Add bias to X_train (and X_val if provided)\n",
        "        # 2. Initialise weights to zeros\n",
        "        # 3. For each iteration:\n",
        "        #    a. Compute gradient\n",
        "        #    b. Update weights\n",
        "        #    c. Record train loss\n",
        "        #    d. Record val loss if X_val provided\n",
        "        pass\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict probabilities.\n",
        "        \n",
        "        Args:\n",
        "            X: Features\n",
        "        \n",
        "        Returns:\n",
        "            Probabilities of class 1\n",
        "        \"\"\"\n",
        "        # TODO: Add bias and compute probabilities using sigmoid (2 points)\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Make class predictions.\n",
        "        \n",
        "        Args:\n",
        "            X: Features\n",
        "            threshold: Classification threshold (default 0.5)\n",
        "        \n",
        "        Returns:\n",
        "            Class predictions (0 or 1)\n",
        "        \"\"\"\n",
        "        # TODO: Get probabilities and apply threshold (2 points)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "logreg_cv_header"
      },
      "source": [
        "## 2.2: 5-Fold Cross-Validation for Logistic Regression (10 points)\n",
        "\n",
        "**Steps:**\n",
        "1. Prepare data: Binary labels (setosa=1, non-setosa=0)\n",
        "2. For each fold:\n",
        "   - Split train into train_inner and validation (80%-20%)\n",
        "   - Grid search over hyperparameters using validation set\n",
        "   - Train final model with best hyperparameters on full train set\n",
        "   - Evaluate on test set\n",
        "3. Report results with mean, std, and 95% CI\n",
        "4. Plot train/val loss curves for one fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "logreg_cv"
      },
      "outputs": [],
      "source": [
        "# Prepare data for Logistic Regression\n",
        "# Binary classification: setosa (0) vs non-setosa (1, 2)\n",
        "X_logreg = X_full.copy()\n",
        "y_logreg = (y_full == 0).astype(int)  # 1 for setosa, 0 for others\n",
        "\n",
        "# Standardise features\n",
        "scaler = StandardScaler()\n",
        "X_logreg = scaler.fit_transform(X_logreg)\n",
        "\n",
        "print(f\"Logistic Regression Data:\")\n",
        "print(f\"  X shape: {X_logreg.shape}\")\n",
        "print(f\"  y shape: {y_logreg.shape}\")\n",
        "print(f\"  Class distribution: {dict(Counter(y_logreg))}\")\n",
        "\n",
        "# Hyperparameter grid\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "reg_strengths = [0.01, 0.1]\n",
        "\n",
        "# Storage for results\n",
        "accuracy_scores = []\n",
        "precision_scores_list = []\n",
        "recall_scores_list = []\n",
        "f1_scores_list = []\n",
        "best_params_per_fold = []\n",
        "\n",
        "# For plotting (save from one fold)\n",
        "plot_train_losses = None\n",
        "plot_val_losses = None\n",
        "\n",
        "# 5-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOGISTIC REGRESSION: 5-FOLD CROSS-VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for fold_num, (train_idx, test_idx) in enumerate(kf.split(X_logreg), 1):\n",
        "    print(f\"\\n--- Fold {fold_num} ---\")\n",
        "    \n",
        "    # Split data\n",
        "    X_train_full, X_test = X_logreg[train_idx], X_logreg[test_idx]\n",
        "    y_train_full, y_test = y_logreg[train_idx], y_logreg[test_idx]\n",
        "    \n",
        "    # TODO: Further split train into train_inner and validation (80%-20%)\n",
        "    \n",
        "    # TODO: Grid search over hyperparameters (3 points)\n",
        "    # For each combination of (learning_rate, reg_strength):\n",
        "    #   1. Train model on train_inner\n",
        "    #   2. Evaluate on validation set (use accuracy for selection)\n",
        "    #   3. Track best parameters\n",
        "    \n",
        "    # TODO: Save train/val losses from one hyperparameter setting for plotting (1 points)\n",
        "    \n",
        "    # TODO: Train final model with best parameters on full training set (1 points)\n",
        "    \n",
        "    # TODO: Evaluate on test set and store results (2 points)\n",
        "    # Store: accuracy, precision, recall, f1\n",
        "    \n",
        "    pass\n",
        "\n",
        "# TODO: Print final results with mean, std, and 95% CI for all metrics (1 points)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOGISTIC REGRESSION: FINAL RESULTS\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "logreg_plot"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot training and validation loss curves (1 points)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "# TODO: Create the plot  (1 points)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question2"
      },
      "source": [
        "**Question 2:** Why do we use the sigmoid function in Logistic Regression? (4 points, 4-5 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "answer2"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "task3_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 3: Decision Tree (30 points)\n",
        "\n",
        "Implement a Decision Tree classifier using information gain (entropy) for multi-class classification of all three Iris species.\n",
        "\n",
        "**Hyperparameters to tune:**\n",
        "- Maximum depth: `[3, 5, 10]`\n",
        "- Minimum samples to split: `[2, 5, 10]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tree_class_header"
      },
      "source": [
        "## 3.1: Implement DecisionTree Class (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tree_class"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeNode:\n",
        "    \"\"\"A node in the decision tree.\"\"\"\n",
        "    \n",
        "    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None):\n",
        "        \"\"\"\n",
        "        Initialise a tree node.\n",
        "        \n",
        "        For internal nodes:\n",
        "            feature_idx: Index of feature to split on\n",
        "            threshold: Threshold value for the split\n",
        "            left: Left child node (feature <= threshold)\n",
        "            right: Right child node (feature > threshold)\n",
        "        \n",
        "        For leaf nodes:\n",
        "            value: Predicted class label\n",
        "        \"\"\"\n",
        "        self.feature_idx = feature_idx\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "    \n",
        "    def is_leaf(self):\n",
        "        \"\"\"Check if this node is a leaf.\"\"\"\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    \"\"\"\n",
        "    Decision Tree classifier using information gain (entropy).\n",
        "    \n",
        "    Attributes:\n",
        "        max_depth: Maximum depth of the tree\n",
        "        min_samples_split: Minimum samples required to split a node\n",
        "        min_samples_leaf: Minimum samples required in a leaf node\n",
        "        root: Root node of the tree\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, max_depth=10, min_samples_split=2, min_samples_leaf=1):\n",
        "        \"\"\"\n",
        "        Initialise the Decision Tree.\n",
        "        \n",
        "        Args:\n",
        "            max_depth: Maximum depth of the tree\n",
        "            min_samples_split: Minimum samples required to split\n",
        "            min_samples_leaf: Minimum samples required in a leaf (fixed 1)\n",
        "        \"\"\"\n",
        "        # TODO: Store hyperparameters (1 points)\n",
        "        pass\n",
        "    \n",
        "    def _entropy(self, y):\n",
        "        \"\"\"\n",
        "        Compute entropy of a label array.\n",
        "        \n",
        "        Entropy = -sum(p * log2(p)) for each class\n",
        "        \n",
        "        Hint: Handle the case where p=0 (0 * log(0) = 0)\n",
        "        \"\"\"\n",
        "        # TODO: Implement entropy calculation (2 points)\n",
        "        pass\n",
        "    \n",
        "    def _information_gain(self, y, y_left, y_right):\n",
        "        \"\"\"\n",
        "        Compute information gain from a split.\n",
        "        \n",
        "        IG = entropy(parent) - weighted_avg(entropy(children))\n",
        "        \"\"\"\n",
        "        # TODO: Implement information gain (2 points)\n",
        "        pass\n",
        "    \n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Find the best feature and threshold to split on.\n",
        "        \n",
        "        Returns:\n",
        "            best_feature_idx, best_threshold, best_gain\n",
        "            Returns (None, None, 0) if no valid split found\n",
        "        \"\"\"\n",
        "        # TODO: Implement best split search (5 points)\n",
        "        # Steps:\n",
        "        # 1. For each feature:\n",
        "        #    a. Get unique values as potential thresholds\n",
        "        #    b. For each threshold:\n",
        "        #       - Split data into left (<=) and right (>)\n",
        "        #       - Check min_samples_leaf constraint\n",
        "        #       - Compute information gain\n",
        "        #       - Track best split\n",
        "        # 2. Return best feature, threshold, and gain\n",
        "        pass\n",
        "    \n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Recursively build the decision tree.\n",
        "        \n",
        "        Args:\n",
        "            X: Features\n",
        "            y: Labels\n",
        "            depth: Current depth\n",
        "        \n",
        "        Returns:\n",
        "            DecisionTreeNode\n",
        "        \"\"\"\n",
        "        # TODO: Implement recursive tree building (6 points)\n",
        "        # Steps:\n",
        "        # 1. Check stopping conditions:\n",
        "        #    - max_depth reached\n",
        "        #    - all samples same class\n",
        "        #    - n_samples < min_samples_split\n",
        "        #    If stopping, return leaf with majority class\n",
        "        #\n",
        "        # 2. Find best split\n",
        "        #    If no valid split, return leaf\n",
        "        #\n",
        "        # 3. Split data and recursively build children\n",
        "        #\n",
        "        # 4. Return internal node with split info\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the decision tree.\n",
        "        \n",
        "        Args:\n",
        "            X: Training features\n",
        "            y: Training labels\n",
        "        \n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        # TODO: Build the tree (2 points)\n",
        "        pass\n",
        "    \n",
        "    def _predict_sample(self, x, node):\n",
        "        \"\"\"\n",
        "        Predict class for a single sample by traversing the tree.\n",
        "        \"\"\"\n",
        "        # TODO: Traverse tree to make prediction (1 points)\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions for all samples.\n",
        "        \n",
        "        Args:\n",
        "            X: Features\n",
        "        \n",
        "        Returns:\n",
        "            Array of predictions\n",
        "        \"\"\"\n",
        "        # TODO: Predict for each sample (1 points)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tree_cv_header"
      },
      "source": [
        "## 3.2: 5-Fold Cross-Validation for Decision Tree (10 points)\n",
        "\n",
        "**Steps:**\n",
        "1. Prepare data: Multi-class classification (all 3 classes)\n",
        "2. For each fold:\n",
        "   - Split train into train_inner and validation (80%-20%)\n",
        "   - Grid search over hyperparameters using validation set\n",
        "   - Train final model with best hyperparameters on full train set\n",
        "   - Evaluate on test set\n",
        "3. Report results with mean, std, and 95% CI\n",
        "4. Plot validation accuracy for different max_depth values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tree_cv"
      },
      "outputs": [],
      "source": [
        "# Prepare data for Decision Tree\n",
        "# Multi-class classification: all 3 classes\n",
        "X_tree = X_full.copy()\n",
        "y_tree = y_full.copy()\n",
        "\n",
        "print(f\"Decision Tree Data:\")\n",
        "print(f\"  X shape: {X_tree.shape}\")\n",
        "print(f\"  y shape: {y_tree.shape}\")\n",
        "print(f\"  Classes: {list(iris.target_names)}\")\n",
        "\n",
        "# Hyperparameter grid\n",
        "max_depths = [3, 5, 10]\n",
        "min_samples_leafs = [1, 2, 4]\n",
        "\n",
        "# Storage for results\n",
        "accuracy_scores = []\n",
        "precision_scores_list = []\n",
        "recall_scores_list = []\n",
        "f1_scores_list = []\n",
        "best_params_per_fold = []\n",
        "\n",
        "# For plotting (track validation accuracy vs max_depth from one fold)\n",
        "depth_val_accuracies = {d: [] for d in max_depths}\n",
        "\n",
        "# 5-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DECISION TREE: 5-FOLD CROSS-VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for fold_num, (train_idx, test_idx) in enumerate(kf.split(X_tree), 1):\n",
        "    print(f\"\\n--- Fold {fold_num} ---\")\n",
        "    \n",
        "    # Split data\n",
        "    X_train_full, X_test = X_tree[train_idx], X_tree[test_idx]\n",
        "    y_train_full, y_test = y_tree[train_idx], y_tree[test_idx]\n",
        "    \n",
        "    # TODO: Further split train into train_inner and validation (80%-20%)\n",
        "    \n",
        "    # TODO: Grid search over hyperparameters (5 points)\n",
        "    # For each combination of (max_depth, min_samples_split, min_samples_leaf):\n",
        "    #   1. Train model on train_inner\n",
        "    #   2. Evaluate on validation set\n",
        "    #   3. Track best parameters\n",
        "    #   4. For plotting: track average val accuracy for each max_depth\n",
        "    \n",
        "    # TODO: Train final model with best parameters on full training set (1 points)\n",
        "    \n",
        "    # TODO: Evaluate on test set and store results (1 points)\n",
        "    # Store: accuracy, precision (macro), recall (macro), f1 (macro)\n",
        "    \n",
        "    pass\n",
        "\n",
        "# TODO: Print final results with mean, std, and 95% CI for all metrics (1 points)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DECISION TREE: FINAL RESULTS\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tree_plot"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot validation accuracy vs max_depth (1 points)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "# TODO: Create the plot (1 points)\n",
        "# plt.plot(max_depths, [np.mean(depth_val_accuracies[d]) for d in max_depths], ...)\n",
        "# plt.xlabel('Max Depth')\n",
        "# plt.ylabel('Validation Accuracy')\n",
        "# plt.title('Decision Tree: Validation Accuracy vs Max Depth')\n",
        "# plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question3"
      },
      "source": [
        "**Question 3:** What are the risks of having a tree that is too shallow or too deep? (4 points, 4-5 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "answer3"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdf_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Convert Your Colab Notebook to PDF\n",
        "\n",
        "### Step 1: Download Your Notebook\n",
        "- Go to **File ‚Üí Download ‚Üí Download .ipynb**\n",
        "- Save the file to your computer\n",
        "\n",
        "### Step 2: Upload to Colab\n",
        "- Click the **üìÅ folder icon** on the left sidebar\n",
        "- Click the **upload button**\n",
        "- Select your downloaded .ipynb file\n",
        "- Wait for the upload to complete\n",
        "\n",
        "### Step 3: Run the Code Below\n",
        "- **Uncomment the cell below** and run the cell\n",
        "- This will take about 1-2 minutes to install required packages\n",
        "\n",
        "### Step 4: Enter Notebook Name\n",
        "- When prompted, type your notebook name (e.g., `gs_000000_project.ipynb`)\n",
        "- Press Enter\n",
        "\n",
        "### The PDF will automatically download to your computer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdf_code"
      },
      "outputs": [],
      "source": [
        "# # Install required packages (this takes about 30 seconds)\n",
        "# print(\"Installing PDF converter... please wait...\")\n",
        "# !apt-get update -qq\n",
        "# !apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic pandoc > /dev/null 2>&1\n",
        "# !pip install -q nbconvert\n",
        "\n",
        "# print(\"\\n\" + \"=\"*50)\n",
        "# print(\"COLAB NOTEBOOK TO PDF CONVERTER\")\n",
        "# print(\"=\"*50)\n",
        "# print(\"\\nSTEP 1: Download your notebook\")\n",
        "# print(\"- Go to File ‚Üí Download ‚Üí Download .ipynb\")\n",
        "# print(\"- Save it to your computer\")\n",
        "# print(\"\\nSTEP 2: Upload it here\")\n",
        "# print(\"- Click the folder icon on the left (üìÅ)\")\n",
        "# print(\"- Click the upload button and select your .ipynb file\")\n",
        "# print(\"- Wait for upload to complete\")\n",
        "# print(\"\\nSTEP 3: Enter the filename below\")\n",
        "# print(\"=\"*50)\n",
        "\n",
        "# # Get notebook name from user\n",
        "# notebook_name = input(\"\\nEnter your notebook name: \")\n",
        "\n",
        "# # Add .ipynb if missing\n",
        "# if not notebook_name.endswith('.ipynb'):\n",
        "#     notebook_name += '.ipynb'\n",
        "\n",
        "# import os\n",
        "# notebook_path = f'/content/{notebook_name}'\n",
        "\n",
        "# # Check if file exists\n",
        "# if not os.path.exists(notebook_path):\n",
        "#     print(f\"\\n‚ö† Error: '{notebook_name}' not found in /content/\")\n",
        "#     print(\"\\nMake sure you uploaded the file using the folder icon (üìÅ) on the left!\")\n",
        "# else:\n",
        "#     print(f\"\\n‚úî Found {notebook_name}\")\n",
        "#     print(\"Converting to PDF... this may take 1-2 minutes...\\n\")\n",
        "\n",
        "#     # Convert the notebook to PDF\n",
        "#     !jupyter nbconvert --to pdf \"{notebook_path}\"\n",
        "\n",
        "#     # Download the PDF\n",
        "#     from google.colab import files\n",
        "#     pdf_name = notebook_name.replace('.ipynb', '.pdf')\n",
        "#     pdf_path = f'/content/{pdf_name}'\n",
        "\n",
        "#     if os.path.exists(pdf_path):\n",
        "#         print(\"‚úî SUCCESS! Downloading your PDF now...\")\n",
        "#         files.download(pdf_path)\n",
        "#         print(\"\\n‚úî Done! Check your downloads folder.\")\n",
        "#     else:\n",
        "#         print(\"‚ö† Error: Could not create PDF\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
